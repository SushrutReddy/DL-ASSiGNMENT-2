# -*- coding: utf-8 -*-
"""DL.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1MBFvC284HX7Mp17y_JnzggtDanMlwVVH

Question 1
"""

import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow.keras.models import Model
from tensorflow.keras.layers import Input, LSTM, Dense

# Function to read and process transliteration data
def load_data(file_path):
    df = pd.read_csv(file_path, sep='\t', header=None).dropna()
    return [(row[1], row[0]) for _, row in df.iterrows()]  # Latin, Devanagari pairs

# Load the train and validation data
train_pairs = load_data('hi.translit.sampled.train.tsv')
val_pairs = load_data('hi.translit.sampled.dev.tsv')

# Prepare sequences with start/end tokens
input_texts = [src for src, _ in train_pairs]
target_texts = ['\t' + tgt + '\n' for _, tgt in train_pairs]

# Create character sets for input and output
input_chars = sorted(set(''.join(input_texts)))
target_chars = sorted(set(''.join(target_texts)))

# Create token-to-index mappings
input_token_index = {ch: i for i, ch in enumerate(input_chars)}
target_token_index = {ch: i for i, ch in enumerate(target_chars)}

# Determine the max sequence lengths
max_encoder_seq_length = max(len(txt) for txt in input_texts)
max_decoder_seq_length = max(len(txt) for txt in target_texts)

# Initialize data arrays for one-hot encoding
encoder_input_data = np.zeros((len(input_texts), max_encoder_seq_length, len(input_chars)), dtype='float32')
decoder_input_data = np.zeros((len(target_texts), max_decoder_seq_length, len(target_chars)), dtype='float32')
decoder_target_data = np.zeros((len(target_texts), max_decoder_seq_length, len(target_chars)), dtype='float32')

# Fill one-hot encoded arrays for input and target sequences
for i, (input_text, target_text) in enumerate(zip(input_texts, target_texts)):
    for t, char in enumerate(input_text):
        encoder_input_data[i, t, input_token_index[char]] = 1.0
    for t, char in enumerate(target_text):
        decoder_input_data[i, t, target_token_index[char]] = 1.0
        if t > 0:
            decoder_target_data[i, t - 1, target_token_index[char]] = 1.0

# Encoder model setup
encoder_inputs = Input(shape=(None, len(input_chars)))
encoder_lstm = LSTM(256, return_state=True)
encoder_outputs, state_h, state_c = encoder_lstm(encoder_inputs)
encoder_states = [state_h, state_c]

# Decoder model setup
decoder_inputs = Input(shape=(None, len(target_chars)))
decoder_lstm = LSTM(256, return_sequences=True, return_state=True)
decoder_outputs, _, _ = decoder_lstm(decoder_inputs, initial_state=encoder_states)
decoder_dense = Dense(len(target_chars), activation='softmax')
decoder_outputs = decoder_dense(decoder_outputs)

# Define and compile the full model
model = Model([encoder_inputs, decoder_inputs], decoder_outputs)
model.compile(optimizer='rmsprop', loss='categorical_crossentropy', metrics=['accuracy'])
model.summary()

# Train the model
model.fit(
    [encoder_input_data, decoder_input_data],
    decoder_target_data,
    batch_size=64,
    epochs=90,
    validation_split=0.22
)

# Save the encoder model for inference
encoder_model = Model(encoder_inputs, encoder_states)

# Define decoder model for inference
decoder_state_input_h = Input(shape=(256,))
decoder_state_input_c = Input(shape=(256,))
decoder_states_inputs = [decoder_state_input_h, decoder_state_input_c]

decoder_outputs, state_h, state_c = decoder_lstm(
    decoder_inputs, initial_state=decoder_states_inputs
)
decoder_states = [state_h, state_c]
decoder_outputs = decoder_dense(decoder_outputs)

decoder_model = Model(
    [decoder_inputs] + decoder_states_inputs,
    [decoder_outputs] + decoder_states
)

# Reverse lookup for character indices
reverse_input_char_index = {i: char for char, i in input_token_index.items()}
reverse_target_char_index = {i: char for char, i in target_token_index.items()}

# Function to decode sequence (input to target)
def decode_sequence(input_seq):
    states_value = encoder_model.predict(input_seq)
    target_seq = np.zeros((1, 1, len(target_chars)))
    target_seq[0, 0, target_token_index['\t']] = 1.0

    decoded_sentence = ''
    for _ in range(max_decoder_seq_length):
        output_tokens, h, c = decoder_model.predict([target_seq] + states_value)
        sampled_token_index = np.argmax(output_tokens[0, -1, :])
        sampled_char = reverse_target_char_index[sampled_token_index]
        decoded_sentence += sampled_char

        if sampled_char == '\n':
            break

        target_seq = np.zeros((1, 1, len(target_chars)))
        target_seq[0, 0, sampled_token_index] = 1.0
        states_value = [h, c]

    return decoded_sentence.strip()

# Test the model on the first 10 sequences
for seq_index in range(10):
    input_seq = encoder_input_data[seq_index:seq_index+1]
    decoded = decode_sequence(input_seq)
    print(f"Input: {input_texts[seq_index]} â†’ Prediction: {decoded} | Target: {target_texts[seq_index]}")

# Function to prepare input text for prediction
def encode_input_text(input_text):
    encoder_input = np.zeros((1, max_encoder_seq_length, len(input_chars)))
    for t, char in enumerate(input_text):
        if char in input_token_index:
            encoder_input[0, t, input_token_index[char]] = 1.0
    return encoder_input

# Interactive loop for user input
while True:
    user_input = input("Enter a Latin word (or type 'exit' to quit): ").strip().lower()
    if user_input == 'exit':
        break

    encoded_input = encode_input_text(user_input)
    prediction = decode_sequence(encoded_input)
    print(f"Predicted Devanagari: {prediction}")











"""Question 2"""

from google.colab import files
files.upload()  # Upload kaggle.json when prompted

!mkdir -p ~/.kaggle
!mv kaggle.json ~/.kaggle/
!chmod 600 ~/.kaggle/kaggle.json

!kaggle datasets download -d paultimothymooney/poetry
!unzip poetry.zip

!pip install transformers datasets accelerate

# Path to the drake.txt file
drake_file_path = './drake.txt'

# Read the file to confirm it's loaded
with open(drake_file_path, 'r', encoding='utf-8') as file:
    drake_lyrics = file.read()

print(drake_lyrics[:500])  # Display the first 500 characters of the lyrics to verify

from transformers import GPT2Tokenizer, GPT2LMHeadModel

# Load the GPT-2 tokenizer and model
tokenizer = GPT2Tokenizer.from_pretrained("gpt2")
tokenizer.pad_token = tokenizer.eos_token

model = GPT2LMHeadModel.from_pretrained("gpt2")
model.resize_token_embeddings(len(tokenizer))

from transformers import TextDataset, DataCollatorForLanguageModeling

# Function to load the dataset (drake.txt) and create a text dataset
def load_dataset(file_path, tokenizer, block_size=128):
    return TextDataset(
        tokenizer=tokenizer,
        file_path=file_path,
        block_size=block_size
    )

# Function to get the data collator for language modeling
def get_data_collator(tokenizer):
    return DataCollatorForLanguageModeling(
        tokenizer=tokenizer,
        mlm=False
    )

# Load the Drake dataset
dataset = load_dataset(drake_file_path, tokenizer)
data_collator = get_data_collator(tokenizer)

from transformers import Trainer, TrainingArguments
import torch

training_args = TrainingArguments(
    output_dir="./gpt2-drake-lyrics",
    overwrite_output_dir=True,
    num_train_epochs=3,
    per_device_train_batch_size=2,
    save_steps=500,
    save_total_limit=2,
    prediction_loss_only=True,
    logging_steps=100,
    fp16=torch.cuda.is_available(),  # Enable mixed precision if GPU is available
    report_to='none',  # Disable W&B logging
)


trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset=dataset,
    data_collator=data_collator,
)

trainer.train()
trainer.save_model("./gpt2-drake-lyrics")
tokenizer.save_pretrained("./gpt2-drake-lyrics")

from transformers import pipeline

# Load the trained model and tokenizer for generation
generator = pipeline("text-generation", model="./gpt2-drake-lyrics", tokenizer="./gpt2-drake-lyrics")

# Set your prompt
prompt = "Started from the bottom, now we're here"
generated_lyrics = generator(prompt, max_length=100, num_return_sequences=1)

# Display the generated lyrics
print(generated_lyrics[0]['generated_text'])

trainer.train()
# Check loss every 100 steps